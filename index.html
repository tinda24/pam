<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="PAM">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="robot manipulation, state ambiguity, adaptive working memory, recoding, dynamic inference">
  <!-- TODO: List all authors -->
  <meta name="author" content="Qingda Hu, Ziheng Qiu">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Magiclab">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>PAM</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/robot.svg">
  <link rel="apple-touch-icon" href="static/images/robot.svg">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/robot.svg",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>



<main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-2 publication-title">
              Resolving State Ambiguity in<br>
              Robot Manipulation<br>
              via Adaptive Working Memory Recoding
            </h1>

            <!-- Authors -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">Qingda Hu<sup>1</sup>,</span>
              <span class="author-block">Ziheng Qiu<sup>1</sup>,</span>
              <span class="author-block">Zijun Xu<sup>1</sup>,</span>
              <span class="author-block">Kaizhao Zhang<sup>1</sup>,</span>
              <span class="author-block">Xizhou Bu<sup>1</sup>,</span>
              <span class="author-block">Zuolei Sun<sup>2</sup>,</span>
              <span class="author-block">Bo Zhang<sup>2</sup>,</span>
              <span class="author-block">Jieru Zhao<sup>3</sup>,</span>
              <span class="author-block">Zhongxue Gan<sup>1</sup>,</span>
              <span class="author-block">and Wenchao Ding<sup>1</sup></span>
            </div>

            <!-- Affiliations -->
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <sup>1</sup> College of intelligent robotics and advanced manufacturing fudan university, Fudan University    <sup>2</sup> Westwell<br>
                <sup>3</sup> Department of Computer Science and Engineering, Shanghai Jiao Tong University, China
              </span>
            </div>

            <!-- Emails -->
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                Email: qdhu24@m.fudan.edu.cn, dingwenchao@fudan.edu.cn
              </span>
            </div>

            <!-- Links -->
            <div class="publication-links">

              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.24638" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>


              <span class="link-block">
                <a href="https://tinda24.github.io/pam/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/pdf/2512.24638" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
</main>



<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser image -->
      <img src="static/images/coarse.png" alt="Teaser Image" style="width: 120%; height: auto;">
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-left">
        <strong style="color: #1a237e;">Left:   </strong> State ambiguity is common in robotic manipulation, and the relevant state transitions do not follow the Markov assumption; therefore, a long history window is essential. Here we illustrate a representative example of state ambiguity and elaborate on its various scenarios. <br>
        <strong style="color: #2e7d32;">Right:   </strong> Existing methods and our method for encoding long history. Analogous to human continuous reasoning, PAMâ€™s inference process is temporally dependent, extending the history window by maintaining a long-term adaptive working memory. The model only needs to encode the current observation at each inference step.       </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Real-World Deployment Videos (1x speed)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div id="results-carousel" class="carousel results-carousel">

        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video1" controls muted loop height="250" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/guess.mp4" type="video/mp4">
          </video>
          <div class="video-caption">
            <p class="has-text-grey is-size-6 has-text-centered">Guessing Game: After the start signal, reveal the red block by lifting the cup.</p>
          </div>
        </div>

        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video2" controls muted loop height="250" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/pothold.mp4" type="video/mp4">
          </video>
          <div class="video-caption">
            <p class="has-text-grey is-size-6 has-text-centered">Hold the Pot Lid: Pick up the pot lid and maintain for 2s before put it back.</p>
          </div>
        </div>

        <div class="item item-video3">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video3" controls muted loop height="250" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/wipetwotime.mp4" type="video/mp4">
          </video>
          <div class="video-caption">
            <p class="has-text-grey is-size-6 has-text-centered">Wipe the Table Twice: Wipe the table for two times with the sponge.</p>
          </div>
        </div>

        <div class="item item-video4">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video4" controls muted loop height="250" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/wipeonetime.mp4" type="video/mp4">
          </video>
          <div class="video-caption">
            <p class="has-text-grey is-size-6 has-text-centered">Wipe the Table Once: Wipe the table for one time with the sponge.</p>
          </div>
        </div>

        <div class="item item-video5">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video4" controls muted loop height="250" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/exchange.mp4" type="video/mp4">
          </video>
          <div class="video-caption">
            <p class="has-text-grey is-size-6 has-text-centered">Exchange Objects: Move the fruit on the board aside and replace it with the other.</p>
          </div>
        </div>

        <div class="item item-video6">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video4" controls muted loop height="250" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/button.mp4" type="video/mp4">
          </video>
          <div class="video-caption">
            <p class="has-text-grey is-size-6 has-text-centered">Buttons in Sequence: Press the buttons in order: Yellow, Green, Yellow and Blue.</p>
          </div>
        </div>

        <div class="item item-video7">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video4" controls muted loop height="250" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/blocks.mp4" type="video/mp4">
          </video>
          <div class="video-caption">
            <p class="has-text-grey is-size-6 has-text-centered">Sponge and Square: Place the sponge and the block on the same side onto board.</p>
          </div>
        </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <div class="content has-text-justified">
      <!-- TODO: Replace with your paper abstract -->
      <p>
        <strong style="color: #1a237e;">State ambiguity</strong> is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting.<br><br>Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce <strong style="color: #1a237e;"><u>P</u>AM, a novel visuomotor <u>P</u>olicy equipped with <u>A</u>daptive working <u>M</u>emory.</strong> With minimal additional training cost in a two-stage manner, PAM supports a <strong style="color: #1a237e;">300-frame</strong> history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately <strong style="color: #1a237e;">10 seconds</strong>, PAM still supports stable training and maintains inference speeds above <strong style="color: #1a237e;">20Hz</strong>.
      </p>
    </div>
  </div>
</section>
<!-- End paper abstract -->






<!-- Pipeline overview -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Interpretability</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <figure class="image">
            <img src="static/images/pipeline.png" alt="PAM Pipeline" style="width: 100%; max-width: 100%;">
            <figcaption class="has-text-grey is-size-6 has-text-left" style="margin-top: 10px;">
              PAM uses two distinct types of features to guide action generation: <strong style="color: #1a237e;">motion primitives</strong> extracted from the
              current frame and <strong style="color: #7b1fa2;">compact context features</strong> drawn from the extended history window. The <strong style="color: #2e7d32;">context features</strong> serve as working memory. (a)
              illustrates the frame feature extractor used to obtain these features. The extractor employs a query-based mechanism to recode multimodal
              inputs, enabling adaptive working memory recoding. (b) illustrates the context router, which receives the the <strong style="color: #2e7d32;">context features</strong> and utilizes
              a set of query tokens spanning different history lengths to produce <strong style="color: #7b1fa2;">compact context features</strong>. PAM is trained in a two-stage manner. As
              shown in the figure, different subsets of model parameters are progressively activated across the two stages.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End pipeline overview -->


<!-- Real-world Robotic Tasks -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Real-world Robotic Tasks</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <figure class="image">
            <img src="static/images/exp.png" alt="Real-world Robotic Tasks" style="width: 100%; max-width: 100%;">
            <figcaption class="has-text-grey is-size-6 has-text-left" style="margin-top: 10px;">
              We carefully designed a set of real-world tasks as shown in the figure, with the primary types of state ambiguity indicated in the top-right corner. Each task comprises multiple subtasks, and to provide a more fine-grained evaluation, the task success rate is computed as the average completion rate across its subtasks.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Real-world Robotic Tasks -->


<!-- Interpretation of PAM -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Interpretability</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <figure class="image">
            <img src="static/images/attn.png" alt="Interpretation of PAM" style="width: 100%; max-width: 100%;">
            <figcaption class="has-text-grey is-size-6 has-text-left" style="margin-top: 10px;">
              <strong>Left:</strong> The attention maps of the context router reveal which history portions PAM references
              to resolve state ambiguity, accurately identifying key frames from preceding task stages in Wipe the Table Twice.<br><br>
              <strong>Right:</strong> The attention maps of extractor indicate which modalities PAM uses for working memory encoding. In Guessing Game, the context query extracts
              historical cues from visual observation of the block's position, while the motion primitives attends to joint states for posture maintenace.
              In Wipe the Table Twice, both visual observations and joint states provide effective contextual cues, demonstrating the effectiveness of
              our adaptive working memory encoding.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End pipeline overview -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{hu2025resolving,
  title={Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding},
  author={Hu, Qingda and Qiu, Ziheng and Xu, Zijun and Zhang, Kaizhao and Bu, Xizhou and Sun, Zuolei and Zhang, Bo and Zhao, Jieru and Gan, Zhongxue and Ding, Wenchao},
  journal={arXiv preprint arXiv:2512.24638},
  year={2025},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
